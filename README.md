# Flow Aggregation Service
## Introduction
The goal of the service is to accept monitoring events, aggregating them and returning these aggregations.

## Requirements
- Write heavy
    - This service is designed to collect events. For each request in a service, there are multiple events generated by 
  single request in a service as each request could result in multiple network calls. These events are grouped and aggregated
  into metrics. Metric are usually read through dashboards or monitors occasionally, for example, to debug issues, to
  understand performance of the system. This results in disproportionate number of calls to Read and Write apis and write
  being heavy and read being light. It is important for this system to be Write efficient which means requires separate
  services to handle read and write api and the service with write api should be scaled enough to handle thousands or 
  millions of requests per second.
- Fire and forget
    - Clients that are submitting events to this service could be sidecar containers submitting network flows or the service
  asynchronously submitting events. These clients need to be quite small, so calls to Write API need to be very quick.
  
- Low write latencies
  - Services publishing events to this service should not be burdened with higher latencies. Essentially there should not 
  be significant impact on the performance of the service publishing events to this service.
- Eventual consistency
  - There could be slight delay between the time event is submitted and the event is available as metric in dashboards.
- Near real time availability of metrics
  - Slight delay must be tolerated between the time event is submitted and the event is available, but the experience of
  the customer should be real time, so the delay should be less than few seconds.
  
## Demo Design
This service exposes 2 APIs: GET /flows?hour=hour and POST /flows. Read API returns list of events aggregated by
subset of flow attributes. Write API accepts list of flows.

Each flow consist of hour, source_app, destination_app, vpc_id, bytes rx and bytes tx. Attributes are divided into 
2 categories, dimensions and metrics. Dimension is the attribute is used to slice and dice multiple types of aggregations
of metrics. In this design, only sum aggregation is supported. Flow attributes hour, source_app, destination_app and vpc_id 
are dimensions and bytes rx and bytes tx are metrics. 

Current implementation is backed by flows db implemented using [Roaring bitmap](https://github.com/RoaringBitmap/RoaringBitmap).
In current implementations, only 2 dimensions are created: hour and combination of source_app, destination_app and vpc_id
referred as flow_key. As part of this demo, Read API is only interested in aggregating by flow_key and filtered by hour, 
which is why 2 dimensions are sufficient.

Table 1

| Hour | Source App | Dest App | Vpc Id  | Bytes Rx | Bytes Tx |     
|------|------------|----------|---------|----------|----------|
| 1    | app1       | app2     | vpc1    | 10       | 11       |
| 1    | app1       | app2     | vpc1    | 20       | 21       |     
| 2    | app1       | app2     | vpc1    | 30       | 31       |    
| 2    | app1       | app2     | vpc1    | 40       | 41       |   
| 2    | app1       | app2     | vpc1    | 50       | 51       |       
| 2    | app1       | app3     | vpc1    | 60       | 61       |      
| 2    | app1       | app3     | vpc1    | 70       | 71       |     
| 2    | app1       | app3     | vpc1    | 80       | 81       |    
| 2    | app2       | app4     | vpc1    | 90       | 91       |   
| 2    | app2       | app4     | vpc1    | 100      | 101      |  

Table 2

| 0     | 1     | 2     | 3     | 4     | 5     | 6     | 7     | 8     | 9       |   
|-------|-------|-------|-------|-------|-------|-------|-------|-------|---------|
| 10,11 | 20,21 | 30,31 | 40,41 | 50,51 | 60,61 | 70,71 | 80,81 | 90,91 | 100,101 |

Table 3

| Hour  | Bitmap (of indices)      |   
|-------|--------------------------|
| 1     | [0, 1]                   |
| 2     | [2, 3, 4, 5, 6, 7, 8, 9] |

Table 4

| Flow Key       | Bitmap (of indices) |   
|----------------|---------------------|
| app1,app2,vpc1 | [0, 1, 2, 3, 4]     |
| app1,app3,vpc1 | [5, 6, 7]           |
| app2,app4,vpc1 | [8, 9]              |

For example, data in table 1 is represented as data structures in table 2, 3 and 4.

## Next
This service is designed to work for querying by hour and grouping by the combination of source_app, destination_app and 
vpc_id. Service can be extended to slice and dice by any combination of hour, source_app, destination_app and vpc_id with
trivial changes to the code base. But extending to grouping by additional combinations of dimensions is limited. We can 
achieve group by any dimension and filter by hour by maintaining multiple bitmaps for every hour. Every hour would have 
bitmap to the dimension values. Dimension values are stored as strings but inorder to maintain bitmaps for 
dimension values, dimension values must be converted to integers.

## Install
```
gradle clean build
java -jar build/libs/flow-aggregation-service-0.0.1-SNAPSHOT.jar
```


